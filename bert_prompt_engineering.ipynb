{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocrgy2iboQu0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBjuUtjcoQu1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import torch as ch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Huggingface imports\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "# Configuration\n",
        "GLUE_TASK_TO_KEYS = {\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}\n",
        "\n",
        "\n",
        "# Adjust dataset size as needed for Colab\n",
        "TRAIN_SET_SIZE = 50_000  # Reduced for Colab memory constraints\n",
        "VAL_SET_SIZE = 5_463\n",
        "\n",
        "\n",
        "class SequenceClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(\n",
        "            \"google-bert/bert-base-cased\",\n",
        "            num_labels=2,\n",
        "            finetuning_task=\"qnli\",\n",
        "            attn_implementation=\"eager\",\n",
        "        )\n",
        "\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"google-bert/bert-base-cased\",\n",
        "            config=self.config,\n",
        "            ignore_mismatched_sizes=False,\n",
        "        )\n",
        "\n",
        "        # Check if GPU is available\n",
        "        self.device = \"cuda\" if ch.cuda.is_available() else \"cpu\"\n",
        "        self.model.eval().to(self.device)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "        return self.model(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        ).logits\n",
        "\n",
        "\n",
        "def get_dataset(split, inds=None):\n",
        "    raw_datasets = load_dataset(\"glue\", \"qnli\")\n",
        "    sentence1_key, sentence2_key = GLUE_TASK_TO_KEYS[\"qnli\"]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"google-bert/bert-base-cased\", use_fast=True\n",
        "    )\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        args = (examples[sentence1_key], examples[sentence2_key])\n",
        "        return tokenizer(*args, padding=\"max_length\", max_length=128, truncation=True)\n",
        "\n",
        "    raw_datasets = raw_datasets.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        desc=\"Running tokenizer on dataset\",\n",
        "    )\n",
        "\n",
        "    if split == \"train\":\n",
        "        ds = raw_datasets[\"train\"]\n",
        "    else:\n",
        "        ds = raw_datasets[\"validation\"]\n",
        "    return ds\n",
        "\n",
        "\n",
        "def init_model(ckpt_path=None):\n",
        "    model = SequenceClassificationModel()\n",
        "    if ckpt_path and os.path.exists(ckpt_path):\n",
        "        sd = ch.load(ckpt_path, map_location=model.device)\n",
        "        model.model.load_state_dict(sd)\n",
        "    return model\n",
        "\n",
        "\n",
        "def init_loaders(batch_size=16):\n",
        "    ds_train = get_dataset(\"train\").select(range(TRAIN_SET_SIZE))\n",
        "    ds_val = get_dataset(\"val\").select(range(VAL_SET_SIZE))\n",
        "    return (\n",
        "        DataLoader(\n",
        "            ds_train,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=default_data_collator,\n",
        "        ),\n",
        "        DataLoader(\n",
        "            ds_val,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=default_data_collator,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def process_batch(batch, device):\n",
        "    return [\n",
        "        x.to(device)\n",
        "        for x in [\n",
        "            batch[\"input_ids\"],\n",
        "            batch[\"token_type_ids\"],\n",
        "            batch[\"attention_mask\"],\n",
        "            batch[\"labels\"],\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = init_model()\n",
        "    train_loader, val_loader = init_loaders(batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87shUoaAoQu1"
      },
      "outputs": [],
      "source": [
        "def predict_custom_text(model, tokenizer, sentence1, sentence2=None):\n",
        "    \"\"\"\n",
        "    Predicts the class of the given custom text input using the model.\n",
        "\n",
        "    Args:\n",
        "        model (SequenceClassificationModel): The sequence classification model.\n",
        "        tokenizer (AutoTokenizer): The tokenizer used for preprocessing text.\n",
        "        sentence1 (str): The first text input (e.g., question or sentence).\n",
        "        sentence2 (str, optional): The second text input (e.g., sentence or hypothesis). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        int: Predicted class label (e.g., 0 or 1).\n",
        "    \"\"\"\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(\n",
        "        sentence1,\n",
        "        sentence2,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
        "\n",
        "    # Perform inference\n",
        "    with ch.no_grad():\n",
        "        logits = model(**inputs)\n",
        "        predictions = ch.argmax(logits, dim=1)  # Get the predicted class\n",
        "\n",
        "    return predictions.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb5bmFHuoQu1"
      },
      "outputs": [],
      "source": [
        "import torch as ch\n",
        "\n",
        "df = pd.read_csv('bert_trak_scores.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym3XjtOkoQu2"
      },
      "outputs": [],
      "source": [
        "def get_context(validation_index):\n",
        "    questions = df[df[\"val_index\"] == validation_index][\"question\"].tolist()\n",
        "    sentences = df[df[\"val_index\"] == validation_index][\"sentence\"].tolist()\n",
        "    labels = df[df[\"val_index\"] == validation_index][\"train_label\"].tolist()\n",
        "\n",
        "    contexts = [f\"{q} {s}: {'ENTAILMENT' if l == 0 else 'NOT_ENTAILMENT'}\" for q, s, l in zip(questions, sentences, labels)]\n",
        "    contexts = \"\\n\".join(contexts)\n",
        "\n",
        "    return contexts\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    correct = 0\n",
        "    correct_with_prompt_engineering = 0\n",
        "    total = 0\n",
        "\n",
        "    data = {\"input\": [], \"prediction\": [], \"prediction_pe\": [], \"label\": []}\n",
        "\n",
        "    with ch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
        "            input_ids, token_type_ids, attention_mask, labels = process_batch(\n",
        "                batch, model.device\n",
        "            )\n",
        "\n",
        "            logits = model(input_ids, token_type_ids, attention_mask)\n",
        "            predictions = ch.argmax(logits, dim=-1)\n",
        "\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"google-bert/bert-base-cased\", use_fast=True\n",
        "            )\n",
        "            input_text = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
        "            labels = labels.cpu().numpy()\n",
        "            predictions = predictions.cpu().numpy()\n",
        "\n",
        "            for i in range(len(input_text)):\n",
        "                validation_index = batch_idx * data_loader.batch_size + i\n",
        "\n",
        "                data[\"input\"].append(input_text[i])\n",
        "                data[\"prediction\"].append(predictions[i].item())\n",
        "                data[\"label\"].append(labels[i].item())\n",
        "\n",
        "                # Check if the prediction is correct with prompt engineering\n",
        "                context = get_context(validation_index)\n",
        "                prediction_pe = predict_custom_text(model, tokenizer, context)\n",
        "                data[\"prediction_pe\"].append(prediction_pe)\n",
        "\n",
        "                if prediction_pe == labels[i]:\n",
        "                    correct_with_prompt_engineering += 1\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(\"bert_predictions.csv\", index=False)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    accuracy_with_prompt_engineering = correct_with_prompt_engineering / total\n",
        "\n",
        "    print(f\"\\nAccuracy without prompt engineering: {accuracy:.4f}\")\n",
        "    print(f\"Accuracy with prompt engineering: {accuracy_with_prompt_engineering:.4f}\")\n",
        "    return accuracy, accuracy_with_prompt_engineering\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Evaluating validation set...\")\n",
        "    val_accuracy, val_accuracy_with_prompt_engineering = evaluate_model(\n",
        "        model, val_loader\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_data = pd.read_csv('bert_predictions.csv')\n",
        "\n",
        "acc = (prediction_data['prediction'] == prediction_data['label']).sum() / len(prediction_data)\n",
        "acc_pe = (prediction_data['prediction_pe'] == prediction_data['label']).sum() / len(prediction_data)\n",
        "\n",
        "print(f\"Accuracy without prompt engineering: {acc}\")\n",
        "print(f\"Accuracy with prompt engineering: {acc_pe}\")"
      ],
      "metadata": {
        "id": "aPZvmZQVui3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}